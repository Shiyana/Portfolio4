<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Artifact 4: Clustering Analysis with K-Means</title>
  <style>
    body { font-family: Arial, sans-serif; background-color: #f4f8fb; color: #333; margin: 0; padding: 0; }
    header, footer { background-color: #1e3799; color: white; text-align: center; padding: 20px; }
    .container { max-width: 1000px; margin: 20px auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
    h1, h2, h3 { color: #1e3799; }
    .tabs { display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 20px; }
    .tabs button { background: #1e3799; color: white; border: none; padding: 10px 15px; border-radius: 5px; cursor: pointer; }
    .tabs button:hover { background: #60a3bc; }
    .tab-content { display: none; }
    .tab-content.active { display: block; }
    .charts img { max-width: 90%; margin: 10px auto; display: block; border: 1px solid #ccc; border-radius: 5px; }
    a { color: #ffffff; text-decoration: underline; }
  </style>
  <script>
    function showTab(id) {
      document.querySelectorAll('.tab-content').forEach(tab => tab.classList.remove('active'));
      document.getElementById(id).classList.add('active');
    }
    document.addEventListener('DOMContentLoaded', () => {
      showTab('overview');
    });
  </script>
</head>
<body>
  <header>
    <h1>Artifact 4: Clustering Analysis with K-Means</h1>
    <p>Shiyana Jayanesan Shylaja | Machine Learning Portfolio</p>
  </header>

  <div class="container">
    <div class="tabs">
      <button onclick="showTab('overview')">Overview</button>
      <button onclick="showTab('process')">Process</button>
      <button onclick="showTab('challenges')">Challenges</button>
      <button onclick="showTab('takeaways')">Key Takeaways</button>
      <button onclick="showTab('reflection')">Reflection</button>
      <button onclick="showTab('conclusion')">Conclusion</button>
    </div>

    <div class="tab-content" id="overview">
      <h2>Overview</h2>
      <p>This section offers a comprehensive introduction to the clustering project, detailing its foundation in K-Means and the real-world problems that inspired it. The experience began with three key chatbot-driven scenarios: (1) dealing with missing data, (2) handling noisy data, and (3) addressing class imbalance. Each scenario was modeled after challenges data professionals regularly face. For missing data, strategies such as mean, median imputation, and advanced methods like KNN imputation were discussed and applied. In the noisy data scenario, smoothing techniques like rolling averages and outlier detection using IQR and z-score were implemented to clean erratic sensor readings. For class imbalance, SMOTE (Synthetic Minority Over-sampling Technique) and stratified sampling were tested for fairness and model balance.

The K-Means portion began with dataset loading, cleaning, and scaling, followed by the use of the Elbow and Silhouette methods to identify an optimal k-value. The final model achieved successful segmentation of customers based on income and spending behavior, validated through visualization. Beyond technical work, ethical aspects—fairness, bias, and leadership in AI—were explored. This artifact highlights a blend of hard and soft skills, showcasing readiness to tackle AI problems through a responsible lens.</p>
    </div>

    <div class="tab-content" id="process">
      <h2>Process</h2>
      <p>The implementation began with importing libraries such as Pandas, NumPy, Matplotlib, Seaborn, and Scikit-learn. The dataset was inspected and cleaned by removing null values and encoding categorical data. Step-by-step preprocessing included normalization using StandardScaler to ensure feature comparability. Exploratory Data Analysis (EDA) identified key patterns and correlations between income and spending score.

We calculated Within-Cluster-Sum-of-Squares (WCSS) for k values ranging from 1 to 10. The Elbow method graph helped select k=3, where marginal returns in WCSS began to diminish. Silhouette scores confirmed this choice, providing an average score above 0.55.

Clustering was then performed using KMeans(n_clusters=3, init='k-means++'). The resulting centroids were plotted and labeled to interpret customer segments. A separate PCA-based visualization was created for dimensionality reduction and clearer visualization. Outputs included: cluster assignments, customer segment statistics, and scatterplots. Further interpretations explained which clusters represented high-spending vs low-spending groups.

This pipeline was fully implemented in Jupyter Notebook, with outputs validated through multiple runs and metrics. Post-clustering, the data was exported for stakeholder presentation and further business intelligence integration.</p>
    </div>

    <div class="tab-content" id="challenges">
      <h2>Challenges & Solutions</h2>
      <p>The project presented a range of technical and conceptual challenges. The first challenge involved data incompleteness. Missing values in key features would have skewed the clustering if not addressed carefully. I experimented with both simple (mean/median) and advanced (KNN) imputations to understand their impacts. Second, the dataset had noticeable outliers—especially in spending scores—that could distort cluster centroids. I mitigated this using z-score filtering and boxplot inspection.

Another major challenge was determining the ideal number of clusters. Relying solely on the Elbow method often introduces subjectivity, so I validated it using Silhouette scores and Davies-Bouldin Index. Interpretability of clusters was yet another hurdle. Clusters are often hard to label, but I cross-referenced cluster centroids with original feature means and developed human-readable labels like "High-Income Moderate-Spenders."

Soft challenges included bias awareness. I had to reflect on whether the clustering may group individuals unfairly based on income or inferred behavior, and I ensured transparency by visualizing all assumptions. Lastly, communicating the output to non-technical audiences required crafting visuals and simple language descriptions. These helped bridge the gap between algorithmic output and real-world impact.</p>
    </div>

    <div class="tab-content" id="takeaways">
      <h2>Key Takeaways</h2>
      <p>This experience sharpened both my technical and strategic understanding. On the technical side, I learned to build a robust K-Means model, including optimal cluster selection using multiple evaluation metrics. I understood the importance of data preprocessing—how scaling and cleaning directly impact the effectiveness of clustering.

I also improved my Python proficiency, particularly in using Scikit-learn, Seaborn, and Matplotlib. Learning how to visualize clusters and interpret results gave me stronger insight into the unsupervised learning pipeline. The experience clarified the distinct roles of KMeans, Silhouette analysis, and PCA in clustering workflows.

From a conceptual and ethical standpoint, I gained a greater appreciation for responsible AI. I learned to question how models make decisions, and to consider the implications of those decisions. This led me to reflect on issues of algorithmic fairness, especially in sensitive domains like healthcare, finance, and education.

Additionally, through chatbot-led scenarios, I strengthened my decision-making and critical thinking. These scenarios challenged me to apply various techniques to imperfect datasets and think like a data scientist under pressure. Finally, I learned how to articulate machine learning processes to stakeholders with diverse technical backgrounds, enhancing my communication skills.</p>
    </div>

    <div class="tab-content" id="reflection">
      <h2>Reflection</h2>
      <p>This artifact has been a transformational journey for me. It combined rigorous coding with real-world complexity and ethical questioning. One of the most significant parts of this experience was the chatbot interaction. By simulating machine learning scenarios, it created an environment where I could test multiple approaches, fail safely, and learn iteratively.

Each scenario—missing data, noisy data, and class imbalance—mirrored actual industry problems. For instance, in a noisy dataset representing sensor data, I used rolling averages and median filters to reduce volatility, while evaluating their effect on cluster consistency. For imbalanced datasets, I explored SMOTE, adjusting sampling strategies to ensure fair representation. These situations helped me develop better instincts and gave me practice in debugging and adjusting models when things don't go as planned.

The integration of leadership and AI ethics added another layer. I started seeing algorithms as tools of influence—not just code. This changed how I looked at fairness, transparency, and user impact. My biggest reflection is realizing that technical excellence alone isn’t enough. As a future AI leader, I must champion inclusivity, test assumptions, and communicate outcomes in ways that empower users rather than confuse or exclude them.</p>
    </div>

    <div class="tab-content" id="conclusion">
      <h2>Conclusion & Future Directions</h2>
      <p>This artifact illustrates the integration of technical mastery and ethical responsibility. K-Means clustering gave me the tools to analyze and interpret customer behavior, while the class activities provided a mirror to reflect on human-centered AI design. Going forward, I plan to explore clustering alternatives like DBSCAN for density-based detection and hierarchical clustering for relationship mapping. I also want to combine clustering with recommendation systems, especially in e-commerce scenarios.

From a tooling perspective, I aim to integrate dimensionality reduction via PCA and t-SNE into my clustering pipeline. I’m also keen to explore cloud-based machine learning tools like AWS SageMaker and Google Vertex AI to scale these efforts in production.

Lastly, I recognize the power of storytelling in AI. No model is useful unless its insights can be shared meaningfully. I will continue working on my communication and visualization skills to ensure that the models I build are accessible, interpretable, and ethically sound. This project has laid a strong foundation, and I’m eager to build upon it with even greater awareness of both the math and the meaning behind the models.</p>
    </div>
  </div>

  <footer>
    <p>&copy; 2025 Shiyana Jayanesan Shylaja |
      <a href="https://sites.google.com/view/shiyana-portfolio/home" target="_blank">Artifact 1</a> |
      <a href="https://shiyana.github.io/Portfolio2/" target="_blank">Artifact 2</a> |
      <a href="https://shiyana.github.io/Portfolio3/" target="_blank">Artifact 3</a> |
      <a href="https://shiyana.github.io/Portfolio4/">Artifact 4</a>
    </p>
  </footer>
</body>
</html>
